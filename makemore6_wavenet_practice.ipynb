{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec472a3-c2c9-4cf9-a021-e109d15ab6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06edd5eb-c73e-4284-b488-8f27824fa2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "encoder:  {'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "decoder:  {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "char_set = ['.'] + sorted(list(set([ch for word in words for ch in list(word)])))\n",
    "\n",
    "stoi = {char: i for i, char in enumerate(char_set)}  #encode(s)\n",
    "itos = {i: char for i, char in enumerate(char_set)}\n",
    "print(\"encoder: \", stoi)\n",
    "print(\"decoder: \", itos)\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468a5cd2-16f2-4a64-9500-6df842e4580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 25],\n",
      "        [ 0,  0,  0,  0,  0,  0, 25, 21],\n",
      "        [ 0,  0,  0,  0,  0, 25, 21,  8],\n",
      "        [ 0,  0,  0,  0, 25, 21,  8,  5],\n",
      "        [ 0,  0,  0, 25, 21,  8,  5, 14],\n",
      "        [ 0,  0, 25, 21,  8,  5, 14,  7],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  4],\n",
      "        [ 0,  0,  0,  0,  0,  0,  4,  9],\n",
      "        [ 0,  0,  0,  0,  0,  4,  9, 15],\n",
      "        [ 0,  0,  0,  0,  4,  9, 15, 14],\n",
      "        [ 0,  0,  0,  4,  9, 15, 14,  4],\n",
      "        [ 0,  0,  4,  9, 15, 14,  4, 18],\n",
      "        [ 0,  4,  9, 15, 14,  4, 18,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 24],\n",
      "        [ 0,  0,  0,  0,  0,  0, 24,  1],\n",
      "        [ 0,  0,  0,  0,  0, 24,  1, 22],\n",
      "        [ 0,  0,  0,  0, 24,  1, 22,  9],\n",
      "        [ 0,  0,  0, 24,  1, 22,  9,  5],\n",
      "        [ 0,  0, 24,  1, 22,  9,  5, 14]])\n",
      "........  ->  y\n",
      ".......y  ->  u\n",
      "......yu  ->  h\n",
      ".....yuh  ->  e\n",
      "....yuhe  ->  n\n",
      "...yuhen  ->  g\n",
      "..yuheng  ->  .\n",
      "........  ->  d\n",
      ".......d  ->  i\n",
      "......di  ->  o\n",
      ".....dio  ->  n\n",
      "....dion  ->  d\n",
      "...diond  ->  r\n",
      "..diondr  ->  e\n",
      ".diondre  ->  .\n",
      "........  ->  x\n",
      ".......x  ->  a\n",
      "......xa  ->  v\n",
      ".....xav  ->  i\n",
      "....xavi  ->  e\n",
      "...xavie  ->  n\n",
      "..xavien  ->  .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        pad_word = '.' * block_size + w + '.'\n",
    "        for i in range(block_size, len(pad_word)):\n",
    "            # print(pad_word[i-block_size:i], \" --> \", pad_word[i])\n",
    "            X.append([stoi[s] for s in pad_word[i-block_size:i]])\n",
    "            Y.append(stoi[pad_word[i]])            \n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "x, y = build_dataset(words[:3])\n",
    "print(x)\n",
    "for i in range(len(x)):\n",
    "    # print(\"\".join([itos[int(ci)] for ci in x[i]]), \" -> \", itos[int(y[i])])\n",
    "    print(\"\".join([itos[ci.item()] for ci in x[i]]), \" -> \", itos[y[i].item()])   # use item() to convert tensor int to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c95c2f-f362-4640-8950-cf6608f0a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f98b08-6d18-4251-b76b-6d11e1dad14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- mimic pytorch.nn ----------\n",
    "# ------------------------------------------------------------\n",
    "class Embedding: # serve as the look up table, for each vocab, embed it\n",
    "  \n",
    "    def __init__(self, num_embeddings, embedding_dim):   # num_embeddings = vocab_size \n",
    "      self.embeds = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, x):   # x shape (batch_size, steps)\n",
    "        self.out = self.embeds[x]  # return  (batch_size, steps, embedding_dim)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.embeds]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "class FlattenConsecutive:  # similar to pytorch Flatten but different dims. \n",
    "    def __init__(self, n_steps):\n",
    "        self.wave_steps = n_steps\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape  # batch_size, steps, embedding_dim\n",
    "        x = x.view(B, T//self.wave_steps, self.wave_steps*C)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)  # if L//self.wave_steps = 1, WE JUST RETURN (B, C*N), squeeze out dim=1\n",
    "        self.out = x    \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # use he kaiming normal distribution as initialization, instead of standard normal ~N(0, 1)\n",
    "        # where weight ~ N (0, std^2)  where std = gain/sqrt(fan_mode) where fan_mode = \"fan_in\" (default) or \"fan_out\"\n",
    "       \n",
    "        self.weights = torch.randn((fan_in, fan_out))/(fan_in)**0.5\n",
    "        # self.bias = None\n",
    "        # if bias:\n",
    "        #     self.bias = torch.zeros(fan_out)\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # if x is passed as batch_size, time_steps, n_embed, flatten it to batch_size * time_steps, n_embed\n",
    "        #x = x.view(-1, fan_in) @ self.weights\n",
    "        # if self.bias:\n",
    "        #     x += self.bias\n",
    "        self.out = x @ self.weights   # dont forget to make self.out, relate out to weights for backprop\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "            \n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        # return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "        if self.bias is None:\n",
    "            return [self.weights]\n",
    "        else:\n",
    "            return [self.weights] + [self.bias]\n",
    "            \n",
    "# -----------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, n_hidden, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum   # for calculating running mean and var\n",
    "        self.training = True   # if not training, no grad on running_mean and var, they are not learnable parameter, but are used during backprop\n",
    "        \n",
    "        '''The mean and standard-deviation are calculated per-dimension over the mini-batches and \n",
    "        γ and β are learnable parameter vectors of size C (where C is the number of features or channels of the input). \n",
    "        By default, the elements of γ are set to 1 and the elements of β are set to 0. \n",
    "        y = ( (x - E(x))/ sqrt(var(x) + eps) ) *  γ +  β \n",
    "        At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). \n",
    "        However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to torch.var(input, unbiased=True).\n",
    "        '''\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(n_hidden)\n",
    "        self.beta = torch.zeros(n_hidden)\n",
    "        \n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(n_hidden)\n",
    "        # wrong: self.running_var = torch.zeros(n_hidden)\n",
    "        self.running_var = torch.ones(n_hidden)\n",
    "        # these are initialized, shared and remain same for one instance of BatchNorm1d,\n",
    "        # xmean and xvar are not self. because they differ with x ???\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            if x.ndim == 2:  # x (batch_size, n_hidden)\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:  # x (batch_size, wave_steps, n_hidden)   note: in pytorch, they use (batch_size, n_hiddden, steps)\n",
    "                dim = (0,1)\n",
    "            # (wrong: not self ): self.x_mean = torch.mean(x, dim=1, keepdim=True)\n",
    "            # (wrong: not self): self.x_var = torch.var(x, dim=1, keepdim=True, unbiased=False)\n",
    "            xmean = x.mean(dim=dim, keepdim=True)  # batch mean\n",
    "            xvar = x.var(dim=dim, keepdim=True)    # batch variation\n",
    "        else: # like for calculate validation loss\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        #self.out = ((x - self.x_mean)/ (self.x_var + self.eps)**0.5 ) * self.gamma + self.beta\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():  # x shape of (batch_size, n_hidden)\n",
    "                self.running_mean = self.running_mean * (1-self.momentum) + xmean * self.momentum\n",
    "                self.running_var = self.running_var * (1-self.momentum) + xvar * self.momentum\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma] + [self.beta]   \n",
    "        # as you can see here xmean and xvar are not parameters, they are torch tensor who participate in backprop, but not parameters,\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "class Tanh:\n",
    "    # def __init__(self):\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864bc03c-96e1-4277-aa1d-fc098da0a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76963\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27\n",
    "embedding_dim = 24\n",
    "n_hidden = 128\n",
    "# wave_steps = block_size\n",
    "wave_steps = 2\n",
    "\n",
    "# note that we have Flatten for all 3 repeated blocks \n",
    "model = Sequential([Embedding(vocab_size, embedding_dim),\n",
    "                    FlattenConsecutive(wave_steps), Linear(embedding_dim * wave_steps, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "                    FlattenConsecutive(wave_steps), Linear(n_hidden * wave_steps, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "                    FlattenConsecutive(wave_steps), Linear(n_hidden * wave_steps, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "                    Linear(n_hidden, vocab_size)]\n",
    "                  )\n",
    "# dont forget the parameters\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weights *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dcb44d-f384-470f-b56d-1d59de80b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3078\n",
      "  10000/ 200000: 1.9507\n",
      "  20000/ 200000: 2.0075\n",
      "  30000/ 200000: 2.0083\n",
      "  40000/ 200000: 1.7478\n",
      "  50000/ 200000: 2.0748\n",
      "  60000/ 200000: 1.8329\n",
      "  70000/ 200000: 1.8637\n",
      "  80000/ 200000: 2.0316\n",
      "  90000/ 200000: 1.6232\n",
      " 100000/ 200000: 1.7761\n",
      " 110000/ 200000: 1.9160\n",
      " 120000/ 200000: 1.8390\n",
      " 130000/ 200000: 1.4186\n",
      " 140000/ 200000: 2.0360\n",
      " 150000/ 200000: 2.1057\n",
      " 160000/ 200000: 1.7750\n",
      " 170000/ 200000: 1.5316\n",
      " 180000/ 200000: 2.3058\n",
      " 190000/ 200000: 1.8961\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # randomly select ix\n",
    "    ix = torch.randint(low =0, high=len(Xtr), size=(batch_size,), generator=g)\n",
    "    x = Xtr[ix]\n",
    "    y = Ytr[ix]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    \n",
    "    # backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        \n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # if i >= 2000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3faa55-9e08-4087-94c7-1d5198ed20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding :  (32, 8, 24)\n",
      "FlattenConsecutive :  (32, 4, 48)\n",
      "Linear :  (32, 4, 128)\n",
      "BatchNorm1d :  (32, 4, 128)\n",
      "Tanh :  (32, 4, 128)\n",
      "FlattenConsecutive :  (32, 2, 256)\n",
      "Linear :  (32, 2, 128)\n",
      "BatchNorm1d :  (32, 2, 128)\n",
      "Tanh :  (32, 2, 128)\n",
      "FlattenConsecutive :  (32, 256)\n",
      "Linear :  (32, 128)\n",
      "BatchNorm1d :  (32, 128)\n",
      "Tanh :  (32, 128)\n",
      "Linear :  (32, 27)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, \": \", tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7df281ef-9bd2-47cc-9ef2-6497374fc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 24])\n",
      "torch.Size([32, 192])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 4, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[0].out.shape) # Embed\n",
    "print(model.layers[1].out.shape) #Flatten\n",
    "print(model.layers[2].out.shape) # Linear\n",
    "\n",
    "# instead of (1,2,3,4,5,6,7,8)\n",
    "# we want (1,2), (3,4), (5,6), (7,8) wave_step_n = 2\n",
    "wave_step_n = 2\n",
    "flat = torch.cat([model.layers[0].out[:, ::2, :], model.layers[0].out[:, 1::2, :]], dim=2)\n",
    "print(flat.shape)\n",
    "\n",
    "(flat == model.layers[0].out.view(batch_size, block_size//wave_step_n, wave_step_n * embedding_dim)).all()\n",
    "\n",
    "# so go back to change Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e0cacdc-3331-4316-8715-347e7d9e595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4, 1, 5, 3, 9, 6, 0, 4, 7, 3, 0, 7, 8, 6, 6, 5, 2, 1, 2, 2, 4, 3, 0,\n",
      "          4, 5, 7, 0, 2, 9, 0, 3, 9, 3, 7, 5, 7, 3, 9, 8, 9, 5, 0, 0, 2, 9, 3,\n",
      "          0, 0],\n",
      "         [9, 0, 4, 4, 0, 0, 6, 5, 4, 2, 5, 6, 3, 0, 3, 5, 7, 3, 9, 4, 5, 1, 7,\n",
      "          2, 1, 3, 9, 0, 2, 1, 8, 2, 0, 1, 8, 4, 4, 4, 1, 3, 1, 6, 9, 2, 4, 4,\n",
      "          4, 4],\n",
      "         [5, 0, 8, 2, 3, 0, 5, 1, 0, 3, 4, 9, 3, 2, 9, 8, 6, 5, 6, 9, 8, 5, 1,\n",
      "          8, 4, 4, 8, 8, 1, 6, 8, 4, 8, 7, 2, 3, 7, 1, 7, 3, 0, 5, 9, 6, 0, 8,\n",
      "          3, 6],\n",
      "         [8, 8, 1, 0, 3, 4, 0, 2, 6, 3, 7, 3, 7, 9, 4, 3, 7, 0, 3, 3, 9, 7, 9,\n",
      "          4, 4, 8, 2, 1, 2, 2, 7, 3, 5, 8, 0, 4, 9, 7, 8, 8, 9, 0, 0, 7, 2, 7,\n",
      "          2, 9]],\n",
      "\n",
      "        [[0, 0, 7, 9, 1, 7, 1, 6, 4, 1, 2, 4, 2, 9, 6, 6, 7, 3, 2, 4, 4, 3, 6,\n",
      "          7, 7, 3, 1, 1, 0, 3, 9, 8, 8, 0, 0, 4, 2, 9, 7, 9, 4, 5, 4, 3, 7, 7,\n",
      "          8, 6],\n",
      "         [1, 1, 5, 9, 7, 6, 5, 6, 7, 7, 8, 7, 6, 5, 3, 0, 3, 2, 0, 1, 4, 2, 2,\n",
      "          0, 1, 6, 3, 4, 0, 0, 2, 0, 7, 5, 5, 9, 0, 1, 2, 5, 0, 3, 5, 6, 1, 9,\n",
      "          8, 7],\n",
      "         [2, 3, 7, 7, 3, 3, 9, 7, 5, 2, 3, 5, 0, 7, 6, 9, 8, 1, 3, 4, 1, 4, 0,\n",
      "          8, 3, 1, 3, 8, 3, 0, 3, 8, 3, 0, 0, 7, 4, 4, 9, 5, 8, 1, 0, 2, 7, 7,\n",
      "          2, 8],\n",
      "         [7, 7, 3, 0, 9, 1, 9, 7, 0, 0, 1, 0, 8, 5, 2, 2, 0, 1, 5, 2, 8, 7, 1,\n",
      "          8, 9, 1, 7, 5, 1, 6, 7, 1, 7, 6, 2, 7, 8, 2, 1, 6, 4, 8, 7, 8, 7, 2,\n",
      "          5, 9]],\n",
      "\n",
      "        [[9, 9, 4, 0, 9, 5, 2, 0, 1, 3, 9, 7, 2, 7, 3, 5, 8, 5, 4, 9, 8, 7, 3,\n",
      "          1, 6, 1, 6, 0, 7, 7, 7, 5, 4, 1, 4, 7, 2, 4, 3, 7, 3, 5, 9, 3, 2, 3,\n",
      "          4, 0],\n",
      "         [4, 5, 2, 1, 0, 5, 0, 4, 4, 1, 2, 3, 3, 7, 7, 1, 2, 3, 0, 5, 6, 7, 2,\n",
      "          3, 8, 5, 0, 6, 0, 5, 6, 7, 5, 1, 5, 5, 0, 1, 7, 8, 3, 4, 3, 2, 0, 7,\n",
      "          2, 3],\n",
      "         [9, 4, 2, 3, 0, 3, 2, 7, 9, 1, 2, 3, 5, 0, 7, 7, 6, 7, 8, 3, 8, 7, 3,\n",
      "          9, 4, 0, 7, 8, 4, 7, 9, 0, 0, 5, 1, 3, 6, 8, 1, 1, 0, 4, 4, 3, 7, 1,\n",
      "          0, 8],\n",
      "         [4, 6, 2, 8, 0, 0, 8, 6, 3, 0, 7, 8, 2, 7, 3, 7, 7, 2, 7, 7, 4, 9, 6,\n",
      "          2, 6, 4, 5, 3, 9, 2, 8, 6, 4, 2, 7, 1, 1, 9, 5, 0, 2, 8, 2, 3, 7, 1,\n",
      "          7, 3]],\n",
      "\n",
      "        [[4, 3, 5, 9, 9, 3, 9, 1, 7, 9, 2, 4, 3, 5, 6, 8, 8, 7, 3, 7, 1, 0, 3,\n",
      "          4, 5, 7, 6, 8, 3, 9, 1, 6, 8, 3, 5, 0, 6, 1, 7, 2, 9, 5, 3, 3, 7, 7,\n",
      "          5, 0],\n",
      "         [8, 2, 4, 1, 7, 5, 5, 3, 3, 2, 0, 6, 0, 7, 7, 1, 5, 9, 8, 5, 7, 9, 7,\n",
      "          4, 7, 6, 6, 4, 9, 8, 3, 5, 2, 8, 9, 3, 7, 8, 1, 3, 8, 4, 8, 1, 5, 0,\n",
      "          9, 6],\n",
      "         [8, 6, 5, 7, 5, 0, 7, 4, 1, 9, 4, 8, 0, 0, 6, 7, 7, 8, 4, 3, 0, 5, 9,\n",
      "          6, 7, 6, 2, 5, 4, 7, 0, 8, 5, 8, 9, 6, 0, 7, 7, 1, 3, 4, 4, 8, 2, 5,\n",
      "          6, 9],\n",
      "         [8, 8, 9, 0, 9, 7, 7, 4, 3, 4, 3, 0, 3, 4, 6, 9, 3, 9, 5, 1, 7, 8, 2,\n",
      "          9, 2, 4, 3, 9, 5, 2, 8, 2, 7, 2, 0, 5, 5, 6, 7, 3, 9, 5, 1, 2, 3, 5,\n",
      "          7, 3]]])\n",
      "tensor([[[4, 8, 2, 1, 2, 2, 7, 3, 5, 8, 0, 4, 9, 7, 8, 8, 9, 0, 0, 7, 2, 7, 2,\n",
      "          9, 4, 1, 5, 3, 9, 6, 0, 4, 7, 3, 0, 7, 8, 6, 6, 5, 2, 1, 2, 2, 4, 3,\n",
      "          0, 4],\n",
      "         [5, 7, 0, 2, 9, 0, 3, 9, 3, 7, 5, 7, 3, 9, 8, 9, 5, 0, 0, 2, 9, 3, 0,\n",
      "          0, 9, 0, 4, 4, 0, 0, 6, 5, 4, 2, 5, 6, 3, 0, 3, 5, 7, 3, 9, 4, 5, 1,\n",
      "          7, 2],\n",
      "         [1, 3, 9, 0, 2, 1, 8, 2, 0, 1, 8, 4, 4, 4, 1, 3, 1, 6, 9, 2, 4, 4, 4,\n",
      "          4, 5, 0, 8, 2, 3, 0, 5, 1, 0, 3, 4, 9, 3, 2, 9, 8, 6, 5, 6, 9, 8, 5,\n",
      "          1, 8],\n",
      "         [4, 4, 8, 8, 1, 6, 8, 4, 8, 7, 2, 3, 7, 1, 7, 3, 0, 5, 9, 6, 0, 8, 3,\n",
      "          6, 8, 8, 1, 0, 3, 4, 0, 2, 6, 3, 7, 3, 7, 9, 4, 3, 7, 0, 3, 3, 9, 7,\n",
      "          9, 4]],\n",
      "\n",
      "        [[9, 1, 7, 5, 1, 6, 7, 1, 7, 6, 2, 7, 8, 2, 1, 6, 4, 8, 7, 8, 7, 2, 5,\n",
      "          9, 0, 0, 7, 9, 1, 7, 1, 6, 4, 1, 2, 4, 2, 9, 6, 6, 7, 3, 2, 4, 4, 3,\n",
      "          6, 7],\n",
      "         [7, 3, 1, 1, 0, 3, 9, 8, 8, 0, 0, 4, 2, 9, 7, 9, 4, 5, 4, 3, 7, 7, 8,\n",
      "          6, 1, 1, 5, 9, 7, 6, 5, 6, 7, 7, 8, 7, 6, 5, 3, 0, 3, 2, 0, 1, 4, 2,\n",
      "          2, 0],\n",
      "         [1, 6, 3, 4, 0, 0, 2, 0, 7, 5, 5, 9, 0, 1, 2, 5, 0, 3, 5, 6, 1, 9, 8,\n",
      "          7, 2, 3, 7, 7, 3, 3, 9, 7, 5, 2, 3, 5, 0, 7, 6, 9, 8, 1, 3, 4, 1, 4,\n",
      "          0, 8],\n",
      "         [3, 1, 3, 8, 3, 0, 3, 8, 3, 0, 0, 7, 4, 4, 9, 5, 8, 1, 0, 2, 7, 7, 2,\n",
      "          8, 7, 7, 3, 0, 9, 1, 9, 7, 0, 0, 1, 0, 8, 5, 2, 2, 0, 1, 5, 2, 8, 7,\n",
      "          1, 8]],\n",
      "\n",
      "        [[6, 4, 5, 3, 9, 2, 8, 6, 4, 2, 7, 1, 1, 9, 5, 0, 2, 8, 2, 3, 7, 1, 7,\n",
      "          3, 9, 9, 4, 0, 9, 5, 2, 0, 1, 3, 9, 7, 2, 7, 3, 5, 8, 5, 4, 9, 8, 7,\n",
      "          3, 1],\n",
      "         [6, 1, 6, 0, 7, 7, 7, 5, 4, 1, 4, 7, 2, 4, 3, 7, 3, 5, 9, 3, 2, 3, 4,\n",
      "          0, 4, 5, 2, 1, 0, 5, 0, 4, 4, 1, 2, 3, 3, 7, 7, 1, 2, 3, 0, 5, 6, 7,\n",
      "          2, 3],\n",
      "         [8, 5, 0, 6, 0, 5, 6, 7, 5, 1, 5, 5, 0, 1, 7, 8, 3, 4, 3, 2, 0, 7, 2,\n",
      "          3, 9, 4, 2, 3, 0, 3, 2, 7, 9, 1, 2, 3, 5, 0, 7, 7, 6, 7, 8, 3, 8, 7,\n",
      "          3, 9],\n",
      "         [4, 0, 7, 8, 4, 7, 9, 0, 0, 5, 1, 3, 6, 8, 1, 1, 0, 4, 4, 3, 7, 1, 0,\n",
      "          8, 4, 6, 2, 8, 0, 0, 8, 6, 3, 0, 7, 8, 2, 7, 3, 7, 7, 2, 7, 7, 4, 9,\n",
      "          6, 2]],\n",
      "\n",
      "        [[2, 4, 3, 9, 5, 2, 8, 2, 7, 2, 0, 5, 5, 6, 7, 3, 9, 5, 1, 2, 3, 5, 7,\n",
      "          3, 4, 3, 5, 9, 9, 3, 9, 1, 7, 9, 2, 4, 3, 5, 6, 8, 8, 7, 3, 7, 1, 0,\n",
      "          3, 4],\n",
      "         [5, 7, 6, 8, 3, 9, 1, 6, 8, 3, 5, 0, 6, 1, 7, 2, 9, 5, 3, 3, 7, 7, 5,\n",
      "          0, 8, 2, 4, 1, 7, 5, 5, 3, 3, 2, 0, 6, 0, 7, 7, 1, 5, 9, 8, 5, 7, 9,\n",
      "          7, 4],\n",
      "         [7, 6, 6, 4, 9, 8, 3, 5, 2, 8, 9, 3, 7, 8, 1, 3, 8, 4, 8, 1, 5, 0, 9,\n",
      "          6, 8, 6, 5, 7, 5, 0, 7, 4, 1, 9, 4, 8, 0, 0, 6, 7, 7, 8, 4, 3, 0, 5,\n",
      "          9, 6],\n",
      "         [7, 6, 2, 5, 4, 7, 0, 8, 5, 8, 9, 6, 0, 7, 7, 1, 3, 4, 4, 8, 2, 5, 6,\n",
      "          9, 8, 8, 9, 0, 9, 7, 7, 4, 3, 4, 3, 0, 3, 4, 6, 9, 3, 9, 5, 1, 7, 8,\n",
      "          2, 9]]])\n",
      "tensor([[[4, 1, 5,  ..., 3, 0, 0],\n",
      "         [9, 0, 4,  ..., 4, 4, 4],\n",
      "         [5, 0, 8,  ..., 8, 3, 6],\n",
      "         ...,\n",
      "         [5, 7, 0,  ..., 1, 7, 2],\n",
      "         [1, 3, 9,  ..., 5, 1, 8],\n",
      "         [4, 4, 8,  ..., 7, 9, 4]],\n",
      "\n",
      "        [[0, 0, 7,  ..., 7, 8, 6],\n",
      "         [1, 1, 5,  ..., 9, 8, 7],\n",
      "         [2, 3, 7,  ..., 7, 2, 8],\n",
      "         ...,\n",
      "         [7, 3, 1,  ..., 2, 2, 0],\n",
      "         [1, 6, 3,  ..., 4, 0, 8],\n",
      "         [3, 1, 3,  ..., 7, 1, 8]],\n",
      "\n",
      "        [[9, 9, 4,  ..., 3, 4, 0],\n",
      "         [4, 5, 2,  ..., 7, 2, 3],\n",
      "         [9, 4, 2,  ..., 1, 0, 8],\n",
      "         ...,\n",
      "         [6, 1, 6,  ..., 7, 2, 3],\n",
      "         [8, 5, 0,  ..., 7, 3, 9],\n",
      "         [4, 0, 7,  ..., 9, 6, 2]],\n",
      "\n",
      "        [[4, 3, 5,  ..., 7, 5, 0],\n",
      "         [8, 2, 4,  ..., 0, 9, 6],\n",
      "         [8, 6, 5,  ..., 5, 6, 9],\n",
      "         ...,\n",
      "         [5, 7, 6,  ..., 9, 7, 4],\n",
      "         [7, 6, 6,  ..., 5, 9, 6],\n",
      "         [7, 6, 2,  ..., 8, 2, 9]]])\n",
      "torch.Size([4, 8, 48])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(10, (4, 8, 24))\n",
    "# print(x)\n",
    "x1 = x.view(4, 4, 48)\n",
    "print(x1)\n",
    "shifted = torch.roll(x, shifts=1, dims=1)\n",
    "# print(shifted)\n",
    "x2 = shifted.view(4, 4, 48)\n",
    "print(x2)\n",
    "print(torch.cat([x1, x2], dim=1))\n",
    "print(torch.cat([x1, x2], dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f52bd34-c2ae-49e2-817a-27088f328e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.7690614461898804\n",
      "val 1.9970234632492065\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    sets = {'train': (Xtr, Ytr), 'val': (Xdev, Ydev), 'test': (Xte, Yte)}\n",
    "    x, y = sets[split]\n",
    "    loss = F.cross_entropy(model(x), y)\n",
    "    print(split, loss.item())\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b76976-a045-4d40-8ae6-2a94b09122ed",
   "metadata": {},
   "source": [
    "# Andrej's performance log\n",
    "original (3 character context + 128 hidden neurons, 47267 params): train 1.989, val 2.078\n",
    "\n",
    "context: 3 -> 8 (62627 params): train 1.918, val 2.027\n",
    "\n",
    "flat -> hierarchical (44195 params): train 1.941, val 2.029\n",
    "\n",
    "fix bug in batchnorm: train 1.912, val 2.022\n",
    "\n",
    "scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9e4388f8-f990-4ce3-bf60-780fb97ec25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(block_size)\n",
    "context = torch.tensor([stoi['.']]*block_size)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eecb511-e47d-4846-9b17-4e321b22e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stellar.\n",
      "yusher.\n",
      "shreed.\n",
      "alleana.\n",
      "kroberto.\n",
      "normah.\n",
      "harlie.\n",
      "ami.\n",
      "taileigh.\n",
      "kaelan.\n",
      "xarney.\n",
      "bownlyn.\n",
      "mossah.\n",
      "addilindsia.\n",
      "shine.\n",
      "claley.\n",
      "wyett.\n",
      "jamila.\n",
      "mamad.\n",
      "artelys.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [stoi['.']]*block_size\n",
    "    # print(\"context: \", context)\n",
    "\n",
    "    while True:\n",
    "        # context = torch.tensor([context])   # first example is context\n",
    "        # print(\"context tensor: \", context)\n",
    "        # print(\"context.shape: \", context.shape)\n",
    "        \n",
    "        logits = model(torch.tensor([context]))   # torch.tensor([context]): shape ([1, 8, 27])  # keep context as a list so we can add & update for next one easily\n",
    "        # print(\"logits.shape: \", logits.shape)   \n",
    "        # training loss: cross_entropy  = - log (softmax(logits))\n",
    "        # wrong: probs = F.softmax(logits) dont forget to choose dimension, on dim=1, [1,8(chosen), 27]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # print(\"probs.shape: \", probs.shape)\n",
    "        #sample_i = torch.multinomial(probs, num_samples=1)\n",
    "        sample_i = torch.multinomial(probs, num_samples=1).item()  # .item() convert to int\n",
    "        # print(\"sample_i: \", sample_i)\n",
    "        \n",
    "        out.append(sample_i)\n",
    "        # don't forget to shift context for next generation\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [sample_i]\n",
    "        # print(\"next context: \", context)\n",
    "        if sample_i == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join([itos[int(sample_i)] for sample_i in out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c96acf-2d6c-466d-94c3-374bd16c764b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
