{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98de3ccd-ca63-42c6-b38e-fc98dfd64d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382673b1-9192-4d06-9b85-de5196f201a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bab72c3-da05-447f-9475-c089fec0d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dac1670-3181-482a-9057-f37d1594ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f28b6acd-1a0f-4ed2-9d33-7fbbca090cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  # print(\"t.grad: \", t.grad)\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f55a54f3-26c6-4d9f-970c-f73354aa9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(214748367) # for reproducibility\n",
    "\n",
    "\n",
    "# write your own C, W1, B1, W2, B2, BN_GAIN  BN_BIAS\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) / (block_size * n_embd) ** 0.5\n",
    "W1 = torch.randn((block_size * n_embd, n_hidden), generator=g) * (5/3) / ((block_size * n_embd) ** 0.5)   # kaiming normal: std = gain/sqrt(fani_in)\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "# standard set-up: b1 = torch.zeros(n_hidden) # or torch.zeros((1, n_hidden))\n",
    "b1 = torch.randn(n_hidden, generator=g) *0.1 # for fun, not needed since we have BN layer\n",
    "\n",
    "# output  after 1st linear is (batch_size, n_hidden), BatchNorm  does average of neuron values along first dimension out.mean(0, keepdim) -> bn_mean.shape (1, n_hidden)\n",
    "#standard way: ones for bngain = torch.ones(n_hidden) # -> broadcast to (1, n_hidden)  when * (out - bnmean)\n",
    "# zeros for bnbias = torch.zeros(n_hidden) \n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0  #make it normal around mean 1\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "# b2 = torch.zeros(vocab_size)\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c1c0aabf-d0bd-4a2b-b47a-9c6ee835b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xb:  tensor([[20,  8,  1],\n",
      "        [ 8,  1, 14]])\n",
      "Yb:  tensor([14,  9])\n",
      "embdflat:  tensor([[-1.6354, -0.3482, -0.6435,  0.9345, -0.1396,  0.7178,  0.4931, -1.4033,\n",
      "          0.9514, -0.7577,  1.7697, -0.5360, -0.2694,  0.8604,  0.3298,  0.8746,\n",
      "         -0.1953,  0.1814, -1.7088,  1.4077,  1.0000,  0.8423, -0.5677,  1.2994,\n",
      "         -0.2922, -0.9407, -0.9639, -1.3883,  0.2883, -0.4113],\n",
      "        [ 1.7697, -0.5360, -0.2694,  0.8604,  0.3298,  0.8746, -0.1953,  0.1814,\n",
      "         -1.7088,  1.4077,  1.0000,  0.8423, -0.5677,  1.2994, -0.2922, -0.9407,\n",
      "         -0.9639, -1.3883,  0.2883, -0.4113,  0.0640,  0.2804,  0.9635,  0.4145,\n",
      "          1.5537, -0.9184,  1.3330, -0.2388,  0.9235, -0.9098]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "h_prebn:  tensor([[-1.2823e+00, -6.9014e-01, -7.5141e-01, -2.0469e+00, -9.2327e-01,\n",
      "          2.0709e+00, -2.1182e+00, -1.4922e+00,  3.7145e+00,  1.2708e+00,\n",
      "         -1.9150e+00, -1.0164e+00, -1.0330e+00, -6.5740e-01,  6.7815e-01,\n",
      "          3.8589e-01,  1.1676e+00,  7.1721e-01,  3.4972e-01,  1.1982e+00,\n",
      "          4.1828e-01,  1.9744e+00, -3.1597e+00, -1.0441e+00, -2.3517e+00,\n",
      "          4.7654e-01, -7.6631e-01, -6.7288e-01,  1.2534e+00, -2.6444e+00,\n",
      "          2.6521e+00,  2.0942e+00,  1.5908e+00,  3.5271e-01,  2.2722e-02,\n",
      "         -5.7769e-02,  1.6054e+00,  1.4166e+00, -1.3762e-01, -5.1792e-01,\n",
      "          1.4203e-01,  3.9907e-01, -1.3641e+00,  2.0749e-01,  1.2612e+00,\n",
      "          1.6209e+00,  2.0021e+00,  1.7819e+00,  2.3837e+00, -7.2541e-01,\n",
      "         -1.8637e+00,  2.4019e+00,  1.6368e+00, -5.0447e-01,  9.6120e-01,\n",
      "          6.1019e-01,  1.2643e+00,  7.9664e-02,  8.0729e-01,  1.9673e-01,\n",
      "          1.1081e+00,  9.2641e-01,  2.0734e+00,  3.6219e+00],\n",
      "        [ 6.6722e-01, -1.4827e+00, -1.8044e+00,  2.1290e+00,  8.5049e-01,\n",
      "          7.5594e-01, -1.6365e+00, -1.2127e+00,  2.6860e-03, -7.7001e-01,\n",
      "         -6.2831e-01, -5.3594e-01, -1.1754e+00,  2.1920e+00, -7.7793e-01,\n",
      "         -4.7754e-01, -4.7821e-01, -1.8038e+00, -1.1582e+00, -2.3338e+00,\n",
      "         -1.9805e+00, -3.6591e-01,  1.8709e+00,  9.4817e-01, -2.9583e+00,\n",
      "         -2.4543e+00,  4.7447e-01, -1.7490e+00,  1.0681e+00, -7.2659e-01,\n",
      "          4.6971e-02,  3.2782e+00, -5.7023e-01,  3.3972e+00,  2.0103e+00,\n",
      "         -2.8498e+00,  1.4742e+00,  7.3684e-01,  4.9619e+00,  1.0764e+00,\n",
      "         -6.7165e-01,  9.6560e-01, -1.5585e+00, -1.1676e+00,  1.7806e+00,\n",
      "         -5.1328e-01,  8.1048e-01,  2.8423e+00, -2.5538e+00, -4.4099e-01,\n",
      "          1.0057e+00,  2.4906e-02,  2.7365e+00, -2.4001e+00,  2.8584e+00,\n",
      "         -6.5351e-01,  2.2162e-01, -8.1771e-01, -2.0391e+00, -1.7281e+00,\n",
      "          3.5666e-01, -3.1361e+00, -2.3746e-01, -4.7032e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bn_out:  tensor([[-0.7955,  0.7872,  0.5037, -0.6623, -0.6181,  0.8684, -0.4544, -0.8316,\n",
      "          0.6692,  0.7104, -0.5973, -0.5633,  0.6876, -0.7835,  0.8305,  0.6327,\n",
      "          0.5961,  0.8157,  0.5778,  0.6744,  0.8638,  0.7240, -0.7299, -0.7090,\n",
      "          0.6328,  0.6888, -0.7567,  0.7695,  0.6325, -0.5324,  0.9499, -0.5648,\n",
      "          0.8166, -0.5771, -0.6295,  0.5938,  0.6147,  0.7189, -0.8029, -0.6870,\n",
      "          0.5994, -0.6189,  0.8055,  0.8398, -0.7240,  0.6402,  0.6618, -0.8954,\n",
      "          0.6992, -0.7520, -0.6368,  0.7467, -0.6681,  0.8161, -0.5439,  0.6374,\n",
      "          0.6816,  0.8341,  0.7881,  0.8159,  0.6604,  0.5934,  0.6132,  0.4757],\n",
      "        [ 0.8054, -0.7663, -0.7409,  0.7039,  0.8411, -0.7091,  0.8675,  0.8967,\n",
      "         -0.7073, -0.8138,  0.7998,  0.5330, -0.9048,  0.5196, -0.4138, -0.6349,\n",
      "         -0.8333, -0.7107, -0.6551, -0.5733, -0.5591, -0.5529,  0.7272,  0.8146,\n",
      "         -0.8032, -0.6611,  0.8213, -0.7402, -0.6144,  0.7825, -0.7966,  0.8141,\n",
      "         -0.5760,  0.8024,  0.7995, -0.8139, -0.8565, -0.7165,  0.7075,  0.8275,\n",
      "         -0.4684,  0.8175, -0.6521, -0.4445,  0.7066, -0.7865, -0.8262,  0.5096,\n",
      "         -0.7772,  0.5330,  0.7640, -0.6696,  0.7018, -0.6632,  0.5763, -0.6776,\n",
      "         -0.7665, -0.7276, -0.4034, -0.5140, -0.8623, -0.6260, -0.7135, -0.7751]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tanh h:  tensor([[-0.6615,  0.6568,  0.4650, -0.5799, -0.5498,  0.7006, -0.4255, -0.6813,\n",
      "          0.5845,  0.6109, -0.5351, -0.5104,  0.5964, -0.6547,  0.6807,  0.5599,\n",
      "          0.5343,  0.6727,  0.5211,  0.5879,  0.6982,  0.6194, -0.6230, -0.6101,\n",
      "          0.5600,  0.5972, -0.6392,  0.6466,  0.5598, -0.4872,  0.7397, -0.5115,\n",
      "          0.6732, -0.5205, -0.5577,  0.5326,  0.5474,  0.6162, -0.6657, -0.5961,\n",
      "          0.5366, -0.5504,  0.6671,  0.6857, -0.6194,  0.5650,  0.5796, -0.7141,\n",
      "          0.6038, -0.6363, -0.5627,  0.6332, -0.5837,  0.6729, -0.4959,  0.5631,\n",
      "          0.5926,  0.6827,  0.6573,  0.6728,  0.5786,  0.5323,  0.5464,  0.4428],\n",
      "        [ 0.6670, -0.6448, -0.6297,  0.6068,  0.6864, -0.6101,  0.7001,  0.7147,\n",
      "         -0.6090, -0.6717,  0.6639,  0.4877, -0.7186,  0.4774, -0.3917, -0.5614,\n",
      "         -0.6822, -0.6111, -0.5751, -0.5178, -0.5073, -0.5027,  0.6213,  0.6721,\n",
      "         -0.6658, -0.5791,  0.6758, -0.6293, -0.5472,  0.6542, -0.6622,  0.6718,\n",
      "         -0.5198,  0.6654,  0.6638, -0.6717, -0.6944, -0.6147,  0.6091,  0.6791,\n",
      "         -0.4369,  0.6737, -0.5731, -0.4174,  0.6086, -0.6564, -0.6784,  0.4696,\n",
      "         -0.6511,  0.4876,  0.6434, -0.5847,  0.6055, -0.5805,  0.5199, -0.5900,\n",
      "         -0.6449, -0.6216, -0.3828, -0.4730, -0.6974, -0.5553, -0.6129, -0.6499]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "logits:  tensor([[ 0.1551, -0.3474, -0.5833,  0.6065,  0.3059, -0.3102, -0.4431, -0.6142,\n",
      "         -0.6800, -0.3743,  1.3371, -0.1919, -0.0723,  0.8697, -0.3051, -0.0608,\n",
      "         -1.4075, -0.5657, -0.0868, -0.2575, -0.1818, -0.6601, -0.0091,  0.2267,\n",
      "          0.1741, -1.0390,  0.3840],\n",
      "        [-0.2256,  0.1045,  0.4564, -0.3682, -0.2731,  0.3195,  0.1974,  0.5307,\n",
      "          0.8200,  0.2982, -1.1185,  0.1460,  0.1314, -0.8948,  0.4133,  0.0977,\n",
      "          0.8631,  0.6171,  0.3005,  0.0902,  0.4515,  0.6701, -0.2932, -0.2054,\n",
      "          0.3244,  0.7374,  0.1158]], grad_fn=<AddBackward0>)\n",
      "softmax(logits):  tensor([[0.0425, 0.0257, 0.0203, 0.0668, 0.0494, 0.0267, 0.0234, 0.0197, 0.0184,\n",
      "         0.0250, 0.1386, 0.0300, 0.0339, 0.0869, 0.0268, 0.0343, 0.0089, 0.0207,\n",
      "         0.0334, 0.0281, 0.0303, 0.0188, 0.0361, 0.0457, 0.0433, 0.0129, 0.0534],\n",
      "        [0.0229, 0.0319, 0.0453, 0.0199, 0.0218, 0.0395, 0.0350, 0.0488, 0.0652,\n",
      "         0.0387, 0.0094, 0.0332, 0.0327, 0.0117, 0.0434, 0.0316, 0.0680, 0.0532,\n",
      "         0.0388, 0.0314, 0.0451, 0.0561, 0.0214, 0.0234, 0.0397, 0.0600, 0.0322]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "target_probs:  tensor([0.0268, 0.0387], grad_fn=<IndexBackward0>)\n",
      "loss:  tensor(3.4356, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.4356, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(214748367) # for reproducibility\n",
    "\n",
    "batch_size = 2\n",
    "# n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "print(\"Xb: \", Xb)\n",
    "print(\"Yb: \", Yb)\n",
    "\n",
    "# fill-in forward pass \n",
    "# embedding layer\n",
    "embd = C[Xb]  # index C.shape[vocab_size, n_embd] by shape [batch_size, block_size] result in [batch_size, block_size, n_embd]\n",
    "embdflat = embd.view(embd.shape[0], -1)\n",
    "print(\"embdflat: \", embdflat)\n",
    "\n",
    "# linear\n",
    "h_prebn = embdflat @ W1 + b1  # shape [batch_size, n_hidden]\n",
    "print(\"h_prebn: \", h_prebn)\n",
    "\n",
    "# # BatchNorm layer\n",
    "# batch norm formular: gain * {(x - E[x]) / sqrt( var(x) + eps )} + bias\n",
    "\n",
    "bn_sumi = h_prebn.sum(0, keepdim=True) # [1, n_hidden], do  not forget divide by n\n",
    "bn_meani = bn_sumi * (batch_size ** -1) # do not forget divide by n\n",
    "\n",
    "# bn_vari = h_prebn.var(0, keepdim=True)\n",
    "# wrong : bn_diff = (h_prebn - bn_meani) * bngain  later *bngain better\n",
    "bn_diff = h_prebn - bn_meani\n",
    "bn_se = bn_diff ** 2\n",
    "bn_sse = bn_se.sum(0, keepdim=True)\n",
    "\n",
    "# # note: Bessel's correction (dividing by n-1, not n),  think of sample variance\n",
    "# sample variance formula: 1/(n-1) * sum of (x - x_mean)^2\n",
    "bn_var = ((batch_size-1)** -1 ) * bn_sse    \n",
    "bn_denominator = (bn_var + 1e-5) ** (-1/2)\n",
    "bn_raw = bn_diff * bn_denominator\n",
    "\n",
    "bn_out = bngain * bn_raw + bnbias\n",
    "print(\"bn_out: \", bn_out)\n",
    "\n",
    "h = torch.tanh(bn_out)\n",
    "print(\"tanh h: \", h)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "print(\"logits: \", logits)\n",
    "\n",
    "####\n",
    "# loss_fast = F.cross_entropy(logits, Yb)\n",
    "# print(\"loss_fast: \", loss_fast) \n",
    "#####\n",
    "# DO NOT FORGET  subtract max \n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "\n",
    "logits_exp = torch.exp(norm_logits)\n",
    "logits_exp_sum = logits_exp.sum(1, keepdim=True)\n",
    "logits_exp_sum_inv = logits_exp_sum**-1\n",
    "logits_likelihood = logits_exp * logits_exp_sum_inv\n",
    "print(\"softmax(logits): \", logits_likelihood)\n",
    "\n",
    "#logits_likelihood[:, Yb]  # wrong\n",
    "\n",
    "##### work, but there's easy eay\n",
    "# Yb_onehot = F.one_hot(Yb, num_classes=vocab_size)\n",
    "# print(\"Yb_onehot: \", Yb_onehot)\n",
    "# prob = logits_likelihood * Yb_onehot\n",
    "# target_probs = prob.max(1, keepdim=True).values\n",
    "#####\n",
    "target_probs = logits_likelihood[range(batch_size), Yb] \n",
    "\n",
    "print(\"target_probs: \", target_probs)\n",
    "target_probs_log = -torch.log(target_probs)\n",
    "\n",
    "loss = target_probs_log.sum() / batch_size\n",
    "print(\"loss: \", loss)  #loss:  tensor(5.8324, grad_fn=<DivBackward0>)\n",
    "\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "    \n",
    "for t in [target_probs_log, target_probs, logits_likelihood, logits_exp_sum_inv, logits_exp_sum, logits_exp, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, bn_out, bn_raw, bn_denominator, bn_var, bn_sse, bn_se, bn_diff,  bn_meani, bn_sumi, \n",
    "          h_prebn, embdflat, embd, C]:\n",
    "  t.retain_grad()\n",
    "    \n",
    "loss.backward(retain_graph=True)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "09bdc716-8112-426a-b21f-288620131d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_probs_log | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "target_probs    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_logits_likelihood | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_logits_exp_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_logits_exp_sum:  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_logits_exp:   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_norm_logits   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_logit_maxes   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True]])\n",
      "d_logits        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_h             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_W2            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bnout         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bngain        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bnbias        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bnraw         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_denominator | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_var        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_sse        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_se         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_diff       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_meani      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_bn_sumi       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_h_prebn       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_embdflat      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_W1            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_embd          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_C             | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# loss = target_probs_log.sum() / batch_size  # [a, b] -> (a+b)/2 -> grad:[0.5, 0.5]\n",
    "d_target_probs_log = torch.full((batch_size,), 1.0/batch_size)\n",
    "# print(\"d_target_probs_log: \", d_target_probs_log)\n",
    "cmp('target_probs_log', d_target_probs_log, target_probs_log)\n",
    "\n",
    "# target_probs_log = -torch.log(target_probs)  dlog(x) = 1/x\n",
    "d_target_probs = - (target_probs**-1) * d_target_probs_log  #  * for element-wise multiplication\n",
    "# print(\"d_target_probs: \", d_target_probs)\n",
    "cmp('target_probs', d_target_probs, target_probs)\n",
    "\n",
    "#target_probs = logits_likelihood[range(batch_size), Yb] # x.grads always have the same shape as x, only probs @ index of Yb have grad, other have no impact \n",
    "d_logits_likelihood = torch.zeros_like(logits_likelihood)\n",
    "d_logits_likelihood[range(batch_size), Yb] = 1.0 * d_target_probs\n",
    "# print(\"d_logits_likelihood: \", d_logits_likelihood)\n",
    "cmp('d_logits_likelihood', d_logits_likelihood, logits_likelihood)\n",
    "\n",
    "# logits_likelihood = logits_exp * logits_exp_sum_inv   # note logits_exp_sum_inv [batch_size, 1] got broadcast here\n",
    "# wrong:d_logits_exp_sum_inv = torch.zeros_like(logits_exp_sum_inv)\n",
    "# d_logits_exp_sum_inv[range(batch_size), :] = logits_exp * d_logits_likelihood  # no need to broadcast here\n",
    "d_logits_exp_sum_inv = (logits_exp  * d_logits_likelihood).sum(1, keepdim=True)\n",
    "# print(\"d_logits_exp_sum_inv: \", d_logits_exp_sum_inv)\n",
    "cmp('d_logits_exp_sum_inv', d_logits_exp_sum_inv, logits_exp_sum_inv)\n",
    "\n",
    "d_logits_exp = logits_exp_sum_inv * d_logits_likelihood # wait for another path to logits_exp\n",
    "\n",
    "# logits_exp_sum_inv = logits_exp_sum**-1\n",
    "d_logits_exp_sum = - (logits_exp_sum ** -2) * d_logits_exp_sum_inv\n",
    "# print(\"d_logits_exp_sum: \", d_logits_exp_sum)\n",
    "cmp(\"d_logits_exp_sum: \", d_logits_exp_sum, logits_exp_sum)\n",
    "\n",
    "# logits_exp_sum = logits_exp.sum(1, keepdim=True)\n",
    "d_logits_exp_here = torch.zeros_like(logits_exp)\n",
    "d_logits_exp_here[:, :] = 1.0 * d_logits_exp_sum\n",
    "d_logits_exp += d_logits_exp_here   # do not forget to combine with the other path \n",
    "# print(\"d_logits_exp: \", d_logits_exp)\n",
    "cmp(\"d_logits_exp: \", d_logits_exp, logits_exp)\n",
    "\n",
    "# logits_exp = torch.exp(norm_logits)\n",
    "d_norm_logits = logits_exp * d_logits_exp\n",
    "# print(\"d_norm_logits: \", d_norm_logits)\n",
    "cmp(\"d_norm_logits\", d_norm_logits, norm_logits)\n",
    "\n",
    "#norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "d_logits = torch.ones_like(logits)\n",
    "#### wrong\n",
    "# d_logits = d_logits * d_norm_logits  # wait for another path to d_logits\n",
    "# d_logit_maxes = torch.zeros_like(logit_maxes)\n",
    "# # print(d_norm_logits[range(batch_size), torch.argmax(logits, 1)])\n",
    "# d_logit_maxes[range(batch_size)] = -1.0 * d_norm_logits[range(batch_size), torch.argmax(logits, 1)].view(batch_size,1)\n",
    "##########\n",
    "# substract logit_maxes at every position, so sum of -1*out_grad\n",
    "d_logit_maxes = (-1 * d_logits *  d_norm_logits).sum(1, keepdim=True) \n",
    "# print(\"d_logits_maxe: \", d_logit_maxes)\n",
    "cmp(\"d_logit_maxes\", d_logit_maxes, logit_maxes)\n",
    "\n",
    "d_logits = torch.ones_like(logits)\n",
    "d_logits *= d_norm_logits\n",
    "print(d_logits == torch.clone(d_norm_logits))\n",
    "\n",
    "#logit_maxes = logits.max(1, keepdim=True).values\n",
    "d_logits_here = torch.zeros_like(logits)\n",
    "d_logits_here[range(batch_size), torch.argmax(logits, 1)] = 1.0\n",
    "# print(\"d_logits_here: \", d_logits_here)\n",
    "d_logits_here  = d_logits_here * d_logit_maxes\n",
    "print(d_logits_here == F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * d_logit_maxes)\n",
    "\n",
    "d_logits += d_logits_here\n",
    "cmp(\"d_logits\", d_logits, logits)  #ha, finally! review: d_logit_maxes was wrong\n",
    "\n",
    "# logits = h @ W2 + b2\n",
    "\n",
    "# h.shape (2, 64) , W2.shape (64, 27)  d_logits.shape(2, 27)  b[27]\n",
    "# example: h = [[a11, a12], [a21, a22]]  W2 = [[w11, w12, w13], [w21, w22, w23]]\n",
    "# [[a11w11+a12w21, a11w12+a12w22, a11w13+a12w23], \n",
    "#  [a21w11+a22w21, a21w12+a22w22, a21w13+a22w23]]\n",
    "# h @ W2 =  shape(2,3)   #  dont forget to chain with outer grad that is d_logits  (2,3) with corresponding grad\n",
    "#[[dlogits11, dlogits12, dlogits13], \n",
    "# [dlogits21, dlogit22, dlogits23]]  \n",
    "\n",
    "# grad w.s.t each position of bn_out # recall that in micrograd and in previous grad to sum, we infuse (cumulate) all grads that arrive at a\n",
    "#[[dlogits11*w11+dlogits12*w12+dlogits13*w13, dlogits11*w21+dlogits12*w22+dlogits13*w23], \n",
    "# [dlogits21*w11+dlogits22*w12+dlogits23*w13, dlogits21*w21+dlogits22*w22+dlogits23*w23]] \n",
    "# =\n",
    "#[[dlogits11, dlogits12, dlogits13],[dlogits21, dlogits22, dlogits23]] * \n",
    "# [[w11, w21],\n",
    "#  [w12, w22],\n",
    "#  [w13, w23]]\n",
    "# which d_logits @ W.T  verify this is the only way to make the shape work (2,27) @ (27, 64)  -> (2,64)\n",
    "d_h = d_logits @ W2.T\n",
    "# print(\"d_h: \", d_h)\n",
    "cmp(\"d_h\", d_h, h)\n",
    "\n",
    "d_W2 = h.T @ d_logits\n",
    "# print(\"d_W2: \", d_W2)\n",
    "cmp(\"d_W2\", d_W2, W2)\n",
    "# b2 is broadcasted in forward, so here we need to sum over all broadcast dimensions \n",
    "# [b1,b2, ...b27]  broadcast vertically for batches [[b1, b2, ...b27],\n",
    "#                                                    [b1, b2,... b27]]\n",
    "# chain with dlogits [[dlogits11, dlogit12, ....dlogits127],\n",
    "#                      [dlogits21, dlogits22, ...dlogits227]]\n",
    "d_b2 = d_logits.sum(0, keepdim=True)\n",
    "cmp(\"d2\", d_b2, b2)\n",
    "\n",
    "#h = torch.tanh(bn_out) d(tanh) = 1-tanh^2\n",
    "d_bnout = (1-h**2) * d_h\n",
    "# print(\"d_bnout: \", d_bnout)\n",
    "cmp(\"d_bnout\", d_bnout, bn_out)\n",
    "\n",
    "# bn_out = bngain * bn_raw + bnbias   # bngain, bnbias shape (1, 64)  broadcast to (2, 64), need to take sum\n",
    "d_bngain = (bn_raw * d_bnout).sum(0, keepdim=True) # note () are nedded\n",
    "cmp(\"d_bngain\", d_bngain, bngain)\n",
    "d_bnbias = d_bnout.sum(0, keepdim=True)\n",
    "cmp(\"d_bnbias\", d_bnbias, bnbias)\n",
    "d_bnraw = bngain * d_bnout\n",
    "cmp(\"d_bnraw\", d_bnraw, bn_raw)\n",
    "\n",
    "# bn_raw = bn_diff * bn_denominator  #bn_diff(2, 64) bn_denominator(1, 64)  d_bnraw (2, 64)\n",
    "d_bn_diff = bn_denominator * d_bnraw  # wait  path bn_se = bn_diff ** 2\n",
    "# cmp(\"d_bn_diff\", d_bn_diff, bn_diff)\n",
    "d_bn_denominator = (bn_diff * d_bnraw).sum(0, keepdim=True)\n",
    "cmp(\"d_bn_denominator\", d_bn_denominator, bn_denominator)\n",
    "\n",
    "# bn_denominator = (bn_var + 1e-5) ** (-1/2)\n",
    "d_bn_var = -0.5 * (bn_var+1e-5)**(-1.5) * 1 * d_bn_denominator  # note chain rule here: bn_var_1e-5 as a whole and 1 wst bn_var\n",
    "cmp(\"d_bn_var\", d_bn_var, bn_var)\n",
    "\n",
    "# bn_var = ((batch_size-1)** -1 ) * bn_sse  \n",
    "d_bn_sse = (batch_size-1)** -1 * d_bn_var\n",
    "cmp(\"d_bn_sse\", d_bn_sse, bn_sse)\n",
    "\n",
    "#bn_sse = bn_se.sum(0, keepdim=True)  #because of sum, local grad @every position of bn_se is 1.0. \n",
    "d_bn_se = torch.ones_like(bn_se) * d_bn_sse\n",
    "cmp(\"d_bn_se\", d_bn_se, bn_se)\n",
    "\n",
    "# bn_se = bn_diff ** 2\n",
    "d_bn_diff += 2 * bn_diff * d_bn_se\n",
    "cmp(\"d_bn_diff\", d_bn_diff, bn_diff)\n",
    "\n",
    "# bn_diff = h_prebn - bn_meani\n",
    "d_h_prebn = 1 * d_bn_diff\n",
    "#  hold on ,cmp(\"d_h_prebn\", d_h_prebn, h_prebn), theres one more path\n",
    "d_bn_meani = (-1 * d_bn_diff).sum(0, keepdim=True) # dont forget this d_bn_meani is (1, 64)\n",
    "cmp(\"d_bn_meani\", d_bn_meani, bn_meani)\n",
    "\n",
    "# bn_meani = bn_sumi * (batch_size ** -1)\n",
    "d_bn_sumi = (batch_size ** -1) * d_bn_meani\n",
    "cmp(\"d_bn_sumi\", d_bn_sumi, bn_sumi)\n",
    "\n",
    "# bn_sumi = h_prebn.sum(0, keepdim=True)\n",
    "d_h_prebn += torch.ones_like(h_prebn) * d_bn_sumi\n",
    "cmp(\"d_h_prebn\", d_h_prebn, h_prebn)\n",
    "\n",
    "# h_prebn = embdflat @ W1 + b1  # here we go again d_h_prebn.shape(2, 64) embdflat.shape(2, 30) W1.shape(30, 64) b1.shape(64)\n",
    "d_embdflat = d_h_prebn @ W1.T  # pay attention here is d_h_prebn, not h_prebn\n",
    "cmp(\"d_embdflat\", d_embdflat, embdflat)\n",
    "\n",
    "d_W1 = embdflat.T @ d_h_prebn\n",
    "cmp(\"d_W1\", d_W1, W1)\n",
    "\n",
    "# embdflat = embd.view(embd.shape[0], -1)  # embd.shape torch.Size([2, 3, 10])  -> (2, 30)\n",
    "d_embd = d_embdflat.view(d_embdflat.shape[0], block_size, -1)\n",
    "cmp(\"d_embd\", d_embd, embd)\n",
    "\n",
    "# embd = C[Xb]  # embd.shape [2, 3, 10]  C.shape[27, 10]  Xb.shape (batch_size, block_size)\n",
    "#C=[ [0.2, 0.8, 0.3],      Xb = [[0,0,1],             d_embd = [ [[0.2, 0.8d, 0.3d], [0.2d, 0.8d, 0.3d], [0.5d , 0.5d, 0.3d]],  \n",
    "   # [0.5, 0.5, 0.3],            [1,2,1]]                      [[0.5d, 0.5d, 0.3d], [0.7d, 0.1d, 0.8d], [0.5d, 0.5d, 0.3d]]]\n",
    "   # [0.7, 0.1, 0.8]]             \n",
    "# vocab_size=3, n_embd=3   batch_size=2 block_size=3\n",
    "\n",
    "# index embd by Xb, Note here for duplicate [0,0,1], corresponding d_embd at index 0 should cumulate\n",
    "d_C = torch.zeros_like(C)\n",
    "for i in range(batch_size):\n",
    "    for step in range(block_size):\n",
    "        ix = Xb[i][step]   # int representing which character in vocab\n",
    "        d_C[ix] += d_embd[i][step]\n",
    "\n",
    "cmp(\"d_C\", d_C, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b76a3-bf0a-4a5b-a064-3f65a729cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cc6cb-1e2f-49c2-b7c8-cf2d230788f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
